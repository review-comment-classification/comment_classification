{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device:/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from matplotlib import pyplot as plt \n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from os import listdir\n",
    "from os.path import isfile\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import os.path\n",
    "from os import path\n",
    "from transformers import TFAutoModel\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "codebert_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#!pip install transformers==4.20.0\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "use_function_metrics = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1463, 365, 1463, 365, 1463, 365)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train = pd.read_excel(folder + 'Train_old.xlsx')\n",
    "# test = pd.read_excel(folder + 'Test_old.xlsx')\n",
    "train = pd.read_excel('Train_new.xlsx')\n",
    "test = pd.read_excel('Test_new.xlsx')\n",
    "train = train[train[\"message\"].notna()]\n",
    "test = test[test[\"message\"].notna()]\n",
    "\n",
    "train_comments = train['message']\n",
    "test_comments = test['message']\n",
    "\n",
    "train_y = train['comment_group'].astype('str')\n",
    "test_y = test['comment_group'].astype('str')\n",
    "len(train),len(test),len(train_comments),len(test_comments),len(train_y),len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df_all = pd.concat([train, test],ignore_index=True)\n",
    "y_df_all = pd.concat([train_y, test_y],ignore_index=True)\n",
    "\n",
    "#X_df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCodes():\n",
    "    codes = []\n",
    "    for comment, comment_id, line_no in zip(X[\"message\"], X['comment_id'], X['line_number']):\n",
    "        line_no = int(line_no)\n",
    "        old_code = None\n",
    "\n",
    "        old_folder = code_folder + comment_id + '/Old/'\n",
    "        if(os.path.isdir(old_folder)):\n",
    "            old_code = \"\"\n",
    "            \n",
    "            for filename in listdir(old_folder):\n",
    "                code_lines = read_code_lines(old_folder, filename, line_no)\n",
    "                lines = \"\\n\".join(code_lines)\n",
    "                old_code = old_code + lines\n",
    "        else:\n",
    "            print(\"no old\")\n",
    "\n",
    "        new_code = None\n",
    "        new_folder = code_folder + comment_id + '/New/'\n",
    "        if(os.path.isdir(new_folder)):\n",
    "            new_code = \"\"\n",
    "            for filename in listdir(new_folder):\n",
    "                code_lines = read_code_lines(new_folder, filename, line_no)\n",
    "                lines = \"\\n\".join(code_lines)\n",
    "                new_code = new_code + lines\n",
    "        else:\n",
    "            print(\"no new\")\n",
    "\n",
    "        codes.append((old_code, new_code))\n",
    "    return codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'codes.pkl'\n",
    "if os.path.exists(file_path):\n",
    "    f = open(file_path, 'rb')\n",
    "    codes = pickle.load(f)\n",
    "else:\n",
    "    codes = getCodes()\n",
    "    f = open(file_path, 'wb')\n",
    "    pickle.dump(codes, f)\n",
    "    # sys.getsizeof(codes)\n",
    "    len(pickle.dumps(codes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''X_df_all.drop(['Unnamed: 0', 'Unnamed: 0.1','Unnamed: 0.1.1', 'project', 'revision_id',\n",
    "            'parent_revision','branch','file_name','reviewer_id','reviewer_name','reviewer_email',\n",
    "            'author_id','Downloaded Patch.1', 'URL','change_id','request_id','line_number','patchset_number' ], axis=1, inplace=True)'''\n",
    "\n",
    "X_df_all.drop(['Unnamed: 0', 'Unnamed: 0.1', 'project', 'revision_id',\n",
    "            'parent_revision','branch','file_name','reviewer_id','reviewer_name','reviewer_email',\n",
    "            'author_id','Downloaded Patch.1', 'URL','change_id','request_id','line_number','patchset_number' ], axis=1, inplace=True)\n",
    "#X_df_all.columns,X_df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_df = pd.DataFrame(codes, columns=['old_code', 'new_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =pd.concat([X_df_all[\"message\"], code_df[\"old_code\"]], axis=\"columns\")\n",
    "#X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(X_df_all['comment_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(model, test_x, y_test, batch_size = 8):\n",
    "    y_preds = model.predict(test_x, batch_size = batch_size)\n",
    "    true_class = tf.argmax( y_test, 1 )\n",
    "    predicted_class = tf.argmax( y_preds, 1 )\n",
    "\n",
    "    # roc_auc = roc_auc_score(true_class, predicted_class, multi_class='ovr')\n",
    "    # print(roc_auc)\n",
    "\n",
    "    print(classification_report(true_class, predicted_class))\n",
    "    cm = confusion_matrix(true_class, predicted_class)\n",
    "    \n",
    "    #ConfusionMatrixDisplay.from_predictions(true_class, predicted_class, display_labels=[str(l) for l in y_test.columns])\n",
    "    return classification_report(true_class, predicted_class, output_dict=True), cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_3x = pd.concat([X, code_df[\"new_code\"]], axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(tokenizer, sentence, total_len):\n",
    "    tokens = tokenizer.encode_plus(sentence,max_length=total_len, truncation=True,add_special_tokens=True, padding=\"max_length\")\n",
    "\n",
    "    return tokens[\"input_ids\"], tokens[\"attention_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_token_slots = 512\n",
    "\n",
    "comment_input_ids = []\n",
    "comment_attention_masks = []\n",
    "old_code_input_ids = []\n",
    "old_code_attention_masks = []\n",
    "new_code_input_ids = []\n",
    "new_code_attention_masks = []\n",
    "for cols, (comment, old_code, new_code) in X_3x.iterrows():\n",
    "    comment = str(comment)\n",
    "    old_code = str(old_code)\n",
    "    new_code = str(new_code)\n",
    "\n",
    "    #input_ids, attention_mask = tokenize_text(bert_tokenizer, comment, total_token_slots)\n",
    "    input_ids, attention_mask = tokenize_text(codebert_tokenizer, comment, total_token_slots)\n",
    "    comment_input_ids.append(input_ids)\n",
    "    comment_attention_masks.append(attention_mask)\n",
    "\n",
    "    input_ids, attention_mask = tokenize_text(codebert_tokenizer, old_code, total_token_slots)\n",
    "    old_code_input_ids.append(input_ids)\n",
    "    old_code_attention_masks.append(attention_mask)\n",
    "\n",
    "    input_ids, attention_mask = tokenize_text(codebert_tokenizer, new_code, total_token_slots)\n",
    "    new_code_input_ids.append(input_ids)\n",
    "    new_code_attention_masks.append(attention_mask)\n",
    "\n",
    "comment_input_ids           = np.array(comment_input_ids)\n",
    "comment_attention_masks     = np.array(comment_attention_masks)\n",
    "old_code_input_ids          = np.array(old_code_input_ids)\n",
    "old_code_attention_masks    = np.array(old_code_attention_masks)\n",
    "new_code_input_ids          = np.array(new_code_input_ids)\n",
    "new_code_attention_masks    = np.array(new_code_attention_masks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_metrics():\n",
    "    metrics_df_path = None\n",
    "    if use_function_metrics:\n",
    "        metrics_df_path = 'data_new_metricsfunctionscope.csv'\n",
    "    else:\n",
    "        metrics_df_path = 'data_new_metrics.csv'\n",
    "        \n",
    "    metrics_df = pd.read_csv(metrics_df_path)\n",
    "\n",
    "    print(\"metrics columns are eqaul check:\", metrics_df['folderName'].equals(X_df_all['comment_id']))\n",
    "    \n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics columns are eqaul check: True\n"
     ]
    }
   ],
   "source": [
    "metrics_df = read_metrics()\n",
    "#metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_chosen_metrics(metrics_df, chosen_float_metric_columns):\n",
    "    metrics_df = metrics_df.merge(X_df_all[chosen_float_metric_columns],left_index =True, right_index=True)\n",
    "    chosen_float_metric_columns = metrics_df.columns \n",
    "    metrics_df = metrics_df.astype(np.float32)\n",
    "    return metrics_df, chosen_float_metric_columns\n",
    "def drop_error_columns(debugColumns):\n",
    "    metrics_df.drop(debugColumns, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_float_metric_columns =['cyclomatic_complexity','comment_loc']\n",
    "\n",
    "metrics_df[\"error\"].replace(np.nan, \"ok\", inplace=True)\n",
    "\n",
    "#dropIndices(metrics_df[metrics_df[\"error\"] != \"ok\"].index)\n",
    "#dropIndices(metrics_df.loc[metrics_df[\"isDupe\"] != 0].index)\n",
    "debugColumns = [\n",
    "        \"folderName\",\n",
    "        # \"hasOldFile\",#these are useful metrics don't drop\n",
    "        # \"hasNewFile\",#these are useful metrics don't drop\n",
    "        \"numOldFiles\",\n",
    "        \"numNewFiles\",\n",
    "        \"error\",\n",
    "        \"isDupe\"\n",
    "    ]\n",
    "\n",
    "drop_error_columns(debugColumns)\n",
    "metrics_df, chosen_metrics_columns = add_chosen_metrics(metrics_df, chosen_float_metric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['anyInserted', 'anyDeleted', 'getMovedSrcs', 'UpdatedSrcs',\n",
      "       'insertedIfConditions', 'deletedIfStmts', 'elseInserted', 'elseDeleted',\n",
      "       'AnythingInLineMoved', 'AnythingInLineUpdated', 'AnythingInLineDeleted',\n",
      "       'AnythingMovedIntoLine', 'AnythingInsertedIntoLine', 'EntireLineMoved',\n",
      "       'EntireLineDeleted', 'stringsUpdated', 'magicStringsReplaced',\n",
      "       'MovedBlocksInIfConditions', 'AddedOrUpdatedComments',\n",
      "       'InsertedAssertConditions', 'InsertedTryCatch',\n",
      "       'UpdatedValueAssignments', 'RemovedTryCatch', 'UpdatedFuncArguments',\n",
      "       'hasOldFile', 'hasNewFile', 'cyclomatic_complexity', 'comment_loc'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(chosen_metrics_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(clip = True)\n",
    "scaler.fit(metrics_df)\n",
    "metrics_df = scaler.transform(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_input_dict(inputDict):\n",
    "    returnDict = dict()\n",
    "    returnDict[\"input_ids\"] = inputDict[\"input_ids\"][dataIndex]\n",
    "    returnDict[\"token_type_ids\"] = inputDict[\"token_type_ids\"][dataIndex]\n",
    "    returnDict[\"attention_mask\"] = inputDict[\"attention_mask\"][dataIndex]\n",
    "    #instance_id =inputDict\n",
    "    return returnDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = {\"comment_input_ids\": comment_input_ids ,\n",
    "              \"comment_attention_mask\": comment_attention_masks,\n",
    "                \"code_input_ids\": old_code_input_ids,\n",
    "                \"code_attention_mask\": old_code_attention_masks,\n",
    "             \"metric\" : metrics_df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = [comment_input_ids, comment_attention_masks,\n",
    "            old_code_input_ids, old_code_attention_masks,\n",
    "            metrics_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_layer_1_name = 'lstm_layer_1'\n",
    "lstm_layer_2_name = \"lstm_layer_2\"\n",
    "bert_layer_name = 'tf_roberta_model'\n",
    "def single_bert_layer_hidden_state(prefix):\n",
    "    input_ids_dummy = tf.keras.layers.Input(shape=(total_token_slots,), name=(prefix+'input_ids'), dtype='int64')\n",
    "    mask_dummy = tf.keras.layers.Input(shape=(total_token_slots,), name=(prefix+'attention_mask'), dtype='int64')\n",
    "    codebert = TFAutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "    # we access the transformer model within our bert object using the bert attribute (eg bert.bert instead of bert)\n",
    "    # https://stackoverflow.com/questions/57000820/how-do-i-convert-bert-embeddings-into-a-tensor-for-feeding-into-an-lstm\n",
    "    # https://github.com/huggingface/transformers/issues/7540#issuecomment-704155218\n",
    "    # Huggingface Bert outputs a tuple, \n",
    "    # [0] => hidden state: (batch_size, seq_len, hidden_size), LSTM layer wants this\n",
    "    # [1] => pooler output: basically just cls token through a tanh function\n",
    "    embeddings_dummy = codebert(input_ids= input_ids_dummy, attention_mask=mask_dummy)[0]\n",
    "\n",
    "    return input_ids_dummy,mask_dummy,codebert,embeddings_dummy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_lstm_text_code():\n",
    "    # two input layers, we ensure layer name variables match to dictionary keys in TF dataset\n",
    "    comment_input_ids_dummy, comment_mask_dummy, comment_codebert, comment_embeddings_dummy = single_bert_layer_hidden_state(\"comment_\")\n",
    "    old_code_input_ids_dummy, old_code_mask_dummy, old_code_codebert, old_code_embeddings_dummy = single_bert_layer_hidden_state(\"code_\")\n",
    "    \n",
    "    comment_x = tf.keras.layers.LSTM(50, dropout=0.3, recurrent_dropout=0.3, name=lstm_layer_1_name)(comment_embeddings_dummy)\n",
    "    old_code_x = tf.keras.layers.LSTM(50, dropout=0.3, recurrent_dropout=0.3, name=lstm_layer_2_name)(old_code_embeddings_dummy)\n",
    "    all_lstm = tf.keras.layers.Concatenate()([comment_x, old_code_x])\n",
    "    \n",
    "    # x = tf.keras.layers.Dense(1024, activation='relu')(embeddings_dummy)\n",
    "    float_metrics_input_dummy = tf.keras.layers.Input(shape=(len(chosen_metrics_columns),), name='metric',dtype='float64')\n",
    "    \n",
    "    x_with_metrics = tf.keras.layers.Concatenate()([all_lstm, float_metrics_input_dummy])\n",
    "    \n",
    "    y = tf.keras.layers.Dense(4, activation='softmax', name='outputs')(x_with_metrics)\n",
    "\n",
    "    # initialize model\n",
    "    model = tf.keras.Model(inputs=[ comment_input_ids_dummy, comment_mask_dummy,\n",
    "                                   old_code_input_ids_dummy, old_code_mask_dummy,\n",
    "                                    float_metrics_input_dummy], outputs=y)\n",
    "\n",
    "    # (optional) freeze bert layer\n",
    "    #model.layers[2].trainable = False\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    acc = tf.keras.metrics.CategoricalAccuracy()\n",
    "    adam_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "    model.compile(loss=loss,optimizer=adam_optimizer,metrics=[acc])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_input_dict(inputDict, dataIndex):\n",
    "    returnDict = dict()\n",
    "    returnDict[\"comment_input_ids\"] = inputDict[\"comment_input_ids\"][dataIndex]\n",
    "    returnDict[\"comment_attention_mask\"] = inputDict[\"comment_attention_mask\"][dataIndex]\n",
    "    returnDict[\"code_input_ids\"] = inputDict[\"code_input_ids\"][dataIndex]\n",
    "    returnDict[\"code_attention_mask\"] = inputDict[\"code_attention_mask\"][dataIndex]\n",
    "    returnDict[\"metric\"] = inputDict[\"metric\"][dataIndex]\n",
    "    #instance_id =inputDict\n",
    "    return returnDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_train_index( length, ratio):\n",
    "    rand_array=np.arange(0,length,1)\n",
    "    train, test =train_test_split(rand_array,test_size=ratio, shuffle=True )\n",
    "    train=np.sort(train)\n",
    "    test=np.sort(test)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=y.values.tolist()\n",
    "#print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/8\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0', 'tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0', 'tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "370/370 [==============================] - 520s 1s/step - loss: 1.2577 - categorical_accuracy: 0.4209 - val_loss: 1.2103 - val_categorical_accuracy: 0.4242\n",
      "Epoch 2/8\n",
      "370/370 [==============================] - 498s 1s/step - loss: 1.0871 - categorical_accuracy: 0.5493 - val_loss: 1.0030 - val_categorical_accuracy: 0.5879\n",
      "Epoch 3/8\n",
      "370/370 [==============================] - 500s 1s/step - loss: 0.8668 - categorical_accuracy: 0.6622 - val_loss: 0.9126 - val_categorical_accuracy: 0.6242\n",
      "Epoch 4/8\n",
      "370/370 [==============================] - 499s 1s/step - loss: 0.6347 - categorical_accuracy: 0.7682 - val_loss: 0.9508 - val_categorical_accuracy: 0.6303\n",
      "Epoch 5/8\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.4482 - categorical_accuracy: 0.8466Restoring model weights from the end of the best epoch: 3.\n",
      "370/370 [==============================] - 500s 1s/step - loss: 0.4482 - categorical_accuracy: 0.8466 - val_loss: 1.0148 - val_categorical_accuracy: 0.6545\n",
      "Epoch 5: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amiangshu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/amiangshu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/amiangshu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/amiangshu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/amiangshu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/amiangshu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        21\n",
      "           1       0.55      0.68      0.61        40\n",
      "           2       0.75      0.62      0.68        72\n",
      "           3       0.51      0.76      0.61        50\n",
      "\n",
      "    accuracy                           0.60       183\n",
      "   macro avg       0.45      0.52      0.48       183\n",
      "weighted avg       0.56      0.60      0.57       183\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/8\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_2/roberta/pooler/dense/kernel:0', 'tf_roberta_model_2/roberta/pooler/dense/bias:0', 'tf_roberta_model_3/roberta/pooler/dense/kernel:0', 'tf_roberta_model_3/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_2/roberta/pooler/dense/kernel:0', 'tf_roberta_model_2/roberta/pooler/dense/bias:0', 'tf_roberta_model_3/roberta/pooler/dense/kernel:0', 'tf_roberta_model_3/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "370/370 [==============================] - 520s 1s/step - loss: 1.1931 - categorical_accuracy: 0.4757 - val_loss: 1.0426 - val_categorical_accuracy: 0.5455\n",
      "Epoch 2/8\n",
      "370/370 [==============================] - 504s 1s/step - loss: 0.9462 - categorical_accuracy: 0.6304 - val_loss: 0.9479 - val_categorical_accuracy: 0.6061\n",
      "Epoch 3/8\n",
      "370/370 [==============================] - 502s 1s/step - loss: 0.7635 - categorical_accuracy: 0.7074 - val_loss: 0.9586 - val_categorical_accuracy: 0.5939\n",
      "Epoch 4/8\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.5438 - categorical_accuracy: 0.8196Restoring model weights from the end of the best epoch: 2.\n",
      "370/370 [==============================] - 503s 1s/step - loss: 0.5438 - categorical_accuracy: 0.8196 - val_loss: 1.0338 - val_categorical_accuracy: 0.5818\n",
      "Epoch 4: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.08      0.14        12\n",
      "           1       0.53      0.56      0.55        43\n",
      "           2       0.64      0.75      0.69        85\n",
      "           3       0.47      0.40      0.43        43\n",
      "\n",
      "    accuracy                           0.58       183\n",
      "   macro avg       0.54      0.45      0.45       183\n",
      "weighted avg       0.57      0.58      0.56       183\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/8\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_4/roberta/pooler/dense/kernel:0', 'tf_roberta_model_4/roberta/pooler/dense/bias:0', 'tf_roberta_model_5/roberta/pooler/dense/kernel:0', 'tf_roberta_model_5/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_4/roberta/pooler/dense/kernel:0', 'tf_roberta_model_4/roberta/pooler/dense/bias:0', 'tf_roberta_model_5/roberta/pooler/dense/kernel:0', 'tf_roberta_model_5/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "370/370 [==============================] - 521s 1s/step - loss: 1.1799 - categorical_accuracy: 0.4797 - val_loss: 1.1992 - val_categorical_accuracy: 0.4848\n",
      "Epoch 2/8\n",
      "370/370 [==============================] - 503s 1s/step - loss: 1.0178 - categorical_accuracy: 0.5973 - val_loss: 1.0347 - val_categorical_accuracy: 0.5818\n",
      "Epoch 3/8\n",
      "370/370 [==============================] - 505s 1s/step - loss: 0.8332 - categorical_accuracy: 0.6709 - val_loss: 0.9939 - val_categorical_accuracy: 0.6364\n",
      "Epoch 4/8\n",
      "370/370 [==============================] - 502s 1s/step - loss: 0.6381 - categorical_accuracy: 0.7757 - val_loss: 1.0172 - val_categorical_accuracy: 0.6182\n",
      "Epoch 5/8\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.4737 - categorical_accuracy: 0.8365Restoring model weights from the end of the best epoch: 3.\n",
      "370/370 [==============================] - 500s 1s/step - loss: 0.4737 - categorical_accuracy: 0.8365 - val_loss: 1.0016 - val_categorical_accuracy: 0.6364\n",
      "Epoch 5: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amiangshu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/amiangshu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/amiangshu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/amiangshu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/amiangshu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/amiangshu/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        15\n",
      "           1       0.60      0.69      0.64        45\n",
      "           2       0.68      0.79      0.73        75\n",
      "           3       0.66      0.60      0.63        48\n",
      "\n",
      "    accuracy                           0.65       183\n",
      "   macro avg       0.48      0.52      0.50       183\n",
      "weighted avg       0.60      0.65      0.62       183\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/8\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_6/roberta/pooler/dense/kernel:0', 'tf_roberta_model_6/roberta/pooler/dense/bias:0', 'tf_roberta_model_7/roberta/pooler/dense/kernel:0', 'tf_roberta_model_7/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_6/roberta/pooler/dense/kernel:0', 'tf_roberta_model_6/roberta/pooler/dense/bias:0', 'tf_roberta_model_7/roberta/pooler/dense/kernel:0', 'tf_roberta_model_7/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "370/370 [==============================] - 519s 1s/step - loss: 1.2175 - categorical_accuracy: 0.4534 - val_loss: 1.1597 - val_categorical_accuracy: 0.5333\n",
      "Epoch 2/8\n",
      "370/370 [==============================] - 500s 1s/step - loss: 1.0179 - categorical_accuracy: 0.5757 - val_loss: 1.1006 - val_categorical_accuracy: 0.5758\n",
      "Epoch 3/8\n",
      "370/370 [==============================] - 500s 1s/step - loss: 0.7970 - categorical_accuracy: 0.6858 - val_loss: 1.0095 - val_categorical_accuracy: 0.6182\n",
      "Epoch 4/8\n",
      "370/370 [==============================] - 500s 1s/step - loss: 0.5618 - categorical_accuracy: 0.8034 - val_loss: 1.2046 - val_categorical_accuracy: 0.5455\n",
      "Epoch 5/8\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.3784 - categorical_accuracy: 0.8811Restoring model weights from the end of the best epoch: 3.\n",
      "370/370 [==============================] - 500s 1s/step - loss: 0.3784 - categorical_accuracy: 0.8811 - val_loss: 1.1841 - val_categorical_accuracy: 0.5879\n",
      "Epoch 5: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.38      0.46        16\n",
      "           1       0.63      0.68      0.65        47\n",
      "           2       0.88      0.69      0.78        85\n",
      "           3       0.49      0.77      0.60        35\n",
      "\n",
      "    accuracy                           0.68       183\n",
      "   macro avg       0.65      0.63      0.62       183\n",
      "weighted avg       0.72      0.68      0.68       183\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/8\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_8/roberta/pooler/dense/kernel:0', 'tf_roberta_model_8/roberta/pooler/dense/bias:0', 'tf_roberta_model_9/roberta/pooler/dense/kernel:0', 'tf_roberta_model_9/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_8/roberta/pooler/dense/kernel:0', 'tf_roberta_model_8/roberta/pooler/dense/bias:0', 'tf_roberta_model_9/roberta/pooler/dense/kernel:0', 'tf_roberta_model_9/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "370/370 [==============================] - 524s 1s/step - loss: 1.2139 - categorical_accuracy: 0.4628 - val_loss: 1.0235 - val_categorical_accuracy: 0.5576\n",
      "Epoch 2/8\n",
      "370/370 [==============================] - 506s 1s/step - loss: 0.9729 - categorical_accuracy: 0.6162 - val_loss: 1.0126 - val_categorical_accuracy: 0.6061\n",
      "Epoch 3/8\n",
      "370/370 [==============================] - 506s 1s/step - loss: 0.7843 - categorical_accuracy: 0.6878 - val_loss: 0.9743 - val_categorical_accuracy: 0.6182\n",
      "Epoch 4/8\n",
      "370/370 [==============================] - 505s 1s/step - loss: 0.5944 - categorical_accuracy: 0.7919 - val_loss: 1.0079 - val_categorical_accuracy: 0.6121\n",
      "Epoch 5/8\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.4001 - categorical_accuracy: 0.8689Restoring model weights from the end of the best epoch: 3.\n",
      "370/370 [==============================] - 506s 1s/step - loss: 0.4001 - categorical_accuracy: 0.8689 - val_loss: 1.0145 - val_categorical_accuracy: 0.6545\n",
      "Epoch 5: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.15      0.24        13\n",
      "           1       0.60      0.74      0.67        43\n",
      "           2       0.75      0.72      0.74        79\n",
      "           3       0.60      0.62      0.61        48\n",
      "\n",
      "    accuracy                           0.66       183\n",
      "   macro avg       0.61      0.56      0.56       183\n",
      "weighted avg       0.66      0.66      0.65       183\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/8\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_10/roberta/pooler/dense/kernel:0', 'tf_roberta_model_10/roberta/pooler/dense/bias:0', 'tf_roberta_model_11/roberta/pooler/dense/kernel:0', 'tf_roberta_model_11/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_10/roberta/pooler/dense/kernel:0', 'tf_roberta_model_10/roberta/pooler/dense/bias:0', 'tf_roberta_model_11/roberta/pooler/dense/kernel:0', 'tf_roberta_model_11/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "370/370 [==============================] - 523s 1s/step - loss: 1.2139 - categorical_accuracy: 0.4669 - val_loss: 1.1147 - val_categorical_accuracy: 0.5515\n",
      "Epoch 2/8\n",
      "370/370 [==============================] - 506s 1s/step - loss: 1.0261 - categorical_accuracy: 0.5791 - val_loss: 1.0922 - val_categorical_accuracy: 0.5455\n",
      "Epoch 3/8\n",
      "370/370 [==============================] - 505s 1s/step - loss: 0.8390 - categorical_accuracy: 0.6791 - val_loss: 1.0394 - val_categorical_accuracy: 0.5818\n",
      "Epoch 4/8\n",
      "370/370 [==============================] - 505s 1s/step - loss: 0.6081 - categorical_accuracy: 0.7899 - val_loss: 1.0365 - val_categorical_accuracy: 0.6667\n",
      "Epoch 5/8\n",
      "370/370 [==============================] - 509s 1s/step - loss: 0.4002 - categorical_accuracy: 0.8689 - val_loss: 1.0795 - val_categorical_accuracy: 0.6848\n",
      "Epoch 6/8\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.2530 - categorical_accuracy: 0.9419Restoring model weights from the end of the best epoch: 4.\n",
      "370/370 [==============================] - 509s 1s/step - loss: 0.2530 - categorical_accuracy: 0.9419 - val_loss: 1.2088 - val_categorical_accuracy: 0.6303\n",
      "Epoch 6: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.15      0.21        13\n",
      "           1       0.61      0.84      0.71        50\n",
      "           2       0.87      0.67      0.76        72\n",
      "           3       0.64      0.71      0.67        48\n",
      "\n",
      "    accuracy                           0.69       183\n",
      "   macro avg       0.61      0.59      0.59       183\n",
      "weighted avg       0.70      0.69      0.68       183\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/8\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_12/roberta/pooler/dense/kernel:0', 'tf_roberta_model_12/roberta/pooler/dense/bias:0', 'tf_roberta_model_13/roberta/pooler/dense/kernel:0', 'tf_roberta_model_13/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_12/roberta/pooler/dense/kernel:0', 'tf_roberta_model_12/roberta/pooler/dense/bias:0', 'tf_roberta_model_13/roberta/pooler/dense/kernel:0', 'tf_roberta_model_13/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "370/370 [==============================] - 524s 1s/step - loss: 1.2183 - categorical_accuracy: 0.4493 - val_loss: 1.0157 - val_categorical_accuracy: 0.6000\n",
      "Epoch 2/8\n",
      "370/370 [==============================] - 506s 1s/step - loss: 1.0018 - categorical_accuracy: 0.6068 - val_loss: 0.9440 - val_categorical_accuracy: 0.6061\n",
      "Epoch 3/8\n",
      "370/370 [==============================] - 508s 1s/step - loss: 0.8031 - categorical_accuracy: 0.6973 - val_loss: 0.8464 - val_categorical_accuracy: 0.6727\n",
      "Epoch 4/8\n",
      "370/370 [==============================] - 508s 1s/step - loss: 0.6415 - categorical_accuracy: 0.7662 - val_loss: 0.9609 - val_categorical_accuracy: 0.6182\n",
      "Epoch 5/8\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.4349 - categorical_accuracy: 0.8554Restoring model weights from the end of the best epoch: 3.\n",
      "370/370 [==============================] - 507s 1s/step - loss: 0.4349 - categorical_accuracy: 0.8554 - val_loss: 0.8892 - val_categorical_accuracy: 0.6970\n",
      "Epoch 5: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.36      0.50        14\n",
      "           1       0.62      0.64      0.63        47\n",
      "           2       0.73      0.73      0.73        82\n",
      "           3       0.49      0.57      0.53        40\n",
      "\n",
      "    accuracy                           0.64       183\n",
      "   macro avg       0.67      0.58      0.60       183\n",
      "weighted avg       0.66      0.64      0.64       183\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/8\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_14/roberta/pooler/dense/kernel:0', 'tf_roberta_model_14/roberta/pooler/dense/bias:0', 'tf_roberta_model_15/roberta/pooler/dense/kernel:0', 'tf_roberta_model_15/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_14/roberta/pooler/dense/kernel:0', 'tf_roberta_model_14/roberta/pooler/dense/bias:0', 'tf_roberta_model_15/roberta/pooler/dense/kernel:0', 'tf_roberta_model_15/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "370/370 [==============================] - 521s 1s/step - loss: 1.2058 - categorical_accuracy: 0.4473 - val_loss: 1.0674 - val_categorical_accuracy: 0.5636\n",
      "Epoch 2/8\n",
      "370/370 [==============================] - 504s 1s/step - loss: 1.0165 - categorical_accuracy: 0.5777 - val_loss: 1.0199 - val_categorical_accuracy: 0.5515\n",
      "Epoch 3/8\n",
      "370/370 [==============================] - 504s 1s/step - loss: 0.8163 - categorical_accuracy: 0.6723 - val_loss: 1.0803 - val_categorical_accuracy: 0.5515\n",
      "Epoch 4/8\n",
      "370/370 [==============================] - ETA: 0s - loss: 0.5932 - categorical_accuracy: 0.7858Restoring model weights from the end of the best epoch: 2.\n",
      "370/370 [==============================] - 503s 1s/step - loss: 0.5932 - categorical_accuracy: 0.7858 - val_loss: 1.0956 - val_categorical_accuracy: 0.5515\n",
      "Epoch 4: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        22\n",
      "           1       0.59      0.57      0.58        47\n",
      "           2       0.53      0.83      0.65        69\n",
      "           3       0.55      0.36      0.43        45\n",
      "\n",
      "    accuracy                           0.55       183\n",
      "   macro avg       0.42      0.44      0.42       183\n",
      "weighted avg       0.49      0.55      0.50       183\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/8\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_16/roberta/pooler/dense/kernel:0', 'tf_roberta_model_16/roberta/pooler/dense/bias:0', 'tf_roberta_model_17/roberta/pooler/dense/kernel:0', 'tf_roberta_model_17/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_16/roberta/pooler/dense/kernel:0', 'tf_roberta_model_16/roberta/pooler/dense/bias:0', 'tf_roberta_model_17/roberta/pooler/dense/kernel:0', 'tf_roberta_model_17/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "371/371 [==============================] - 724s 2s/step - loss: 1.2186 - categorical_accuracy: 0.4517 - val_loss: 1.0798 - val_categorical_accuracy: 0.5515\n",
      "Epoch 2/8\n",
      "371/371 [==============================] - 701s 2s/step - loss: 0.9831 - categorical_accuracy: 0.6050 - val_loss: 1.1107 - val_categorical_accuracy: 0.5515\n",
      "Epoch 3/8\n",
      "371/371 [==============================] - 693s 2s/step - loss: 0.7978 - categorical_accuracy: 0.6941 - val_loss: 1.0514 - val_categorical_accuracy: 0.5818\n",
      "Epoch 4/8\n",
      "371/371 [==============================] - 696s 2s/step - loss: 0.5904 - categorical_accuracy: 0.7927 - val_loss: 1.1359 - val_categorical_accuracy: 0.6182\n",
      "Epoch 5/8\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.4010 - categorical_accuracy: 0.8805Restoring model weights from the end of the best epoch: 3.\n",
      "371/371 [==============================] - 697s 2s/step - loss: 0.4010 - categorical_accuracy: 0.8805 - val_loss: 1.2384 - val_categorical_accuracy: 0.5879\n",
      "Epoch 5: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.18      0.26        17\n",
      "           1       0.60      0.68      0.64        41\n",
      "           2       0.70      0.76      0.73        80\n",
      "           3       0.33      0.32      0.33        44\n",
      "\n",
      "    accuracy                           0.58       182\n",
      "   macro avg       0.53      0.49      0.49       182\n",
      "weighted avg       0.57      0.58      0.57       182\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at microsoft/codebert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_layer_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_layer_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/8\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_18/roberta/pooler/dense/kernel:0', 'tf_roberta_model_18/roberta/pooler/dense/bias:0', 'tf_roberta_model_19/roberta/pooler/dense/kernel:0', 'tf_roberta_model_19/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_18/roberta/pooler/dense/kernel:0', 'tf_roberta_model_18/roberta/pooler/dense/bias:0', 'tf_roberta_model_19/roberta/pooler/dense/kernel:0', 'tf_roberta_model_19/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "371/371 [==============================] - 727s 2s/step - loss: 1.2328 - categorical_accuracy: 0.4396 - val_loss: 1.1653 - val_categorical_accuracy: 0.5030\n",
      "Epoch 2/8\n",
      "371/371 [==============================] - 701s 2s/step - loss: 1.0509 - categorical_accuracy: 0.5820 - val_loss: 1.0996 - val_categorical_accuracy: 0.5152\n",
      "Epoch 3/8\n",
      "371/371 [==============================] - 701s 2s/step - loss: 0.8563 - categorical_accuracy: 0.6793 - val_loss: 1.1023 - val_categorical_accuracy: 0.5273\n",
      "Epoch 4/8\n",
      "371/371 [==============================] - 706s 2s/step - loss: 0.6839 - categorical_accuracy: 0.7461 - val_loss: 1.0968 - val_categorical_accuracy: 0.6061\n",
      "Epoch 5/8\n",
      "371/371 [==============================] - 705s 2s/step - loss: 0.5217 - categorical_accuracy: 0.8244 - val_loss: 1.1167 - val_categorical_accuracy: 0.6182\n",
      "Epoch 6/8\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.3660 - categorical_accuracy: 0.8805Restoring model weights from the end of the best epoch: 4.\n",
      "371/371 [==============================] - 701s 2s/step - loss: 0.3660 - categorical_accuracy: 0.8805 - val_loss: 1.2888 - val_categorical_accuracy: 0.5697\n",
      "Epoch 6: early stopping\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.27      0.32        15\n",
      "           1       0.67      0.52      0.59        42\n",
      "           2       0.68      0.86      0.76        85\n",
      "           3       0.68      0.53      0.59        40\n",
      "\n",
      "    accuracy                           0.66       182\n",
      "   macro avg       0.61      0.54      0.56       182\n",
      "weighted avg       0.65      0.66      0.65       182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#five_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED) \n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "precision = [0.0, 0.0, 0.0, 0.0]\n",
    "recall = [0.0, 0.0, 0.0, 0.0]\n",
    "f_score = [0.0, 0.0, 0.0, 0.0]\n",
    "accuracy = 0.0\n",
    "\n",
    "rows, cols = (4, 4)\n",
    "confusion = [[0.0]*cols]*rows\n",
    "\n",
    "random_folding = KFold(n_splits=10, shuffle=True, random_state=SEED)\n",
    "for train_index, test_index in random_folding.split(y):\n",
    "    \n",
    "    train_input_set = partition_input_dict(data_input, train_index)\n",
    "    test_input = partition_input_dict(data_input, test_index)\n",
    "    train_output = y[train_index,]\n",
    "    test_output = y[test_index,]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    training_index, validation_index = generate_test_train_index(train_output.shape[0], 0.1)\n",
    "\n",
    "    model_training_input = partition_input_dict(train_input_set, training_index)\n",
    "\n",
    "    #print(len(model_training_input))\n",
    "\n",
    "    validation_input = partition_input_dict(train_input_set, validation_index)\n",
    "    model_training_output = train_output[training_index]\n",
    "    validation_output = train_output[validation_index]\n",
    "            \n",
    "    \n",
    "   # X_trval_fold, X_test_fold = X_all[train_index], X_all[test_index] \n",
    "   # y_trval_fold, y_test_fold = y[train_index], y[test_index]\n",
    "    \n",
    "    '''X_train_fold, X_val_fold, y_train_fold, y_val_fold = train_test_split(X_trval_fold, y_trval_fold,\n",
    "                                   random_state=SEED, \n",
    "                                   test_size=0.11,\n",
    "                                   shuffle=True)'''\n",
    "    \n",
    "    model = get_bert_lstm_text_code()\n",
    "    #model.summary()\n",
    "    callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        min_delta=0,\n",
    "        patience=2,\n",
    "        verbose=1,\n",
    "        mode=\"min\",\n",
    "        baseline=None,\n",
    "        restore_best_weights=True,)\n",
    "    \n",
    "    history = model.fit(model_training_input,\n",
    "                        model_training_output, \n",
    "                        batch_size=4,\n",
    "                        epochs=8,\n",
    "                        validation_data=(validation_input, validation_output),\n",
    "                        callbacks=[callback],\n",
    "                        )\n",
    "    res, confusion_mat = metrics(model, test_input, test_output)\n",
    "    #print(res['1']['precision'])\n",
    "    \n",
    "    confusion += confusion_mat\n",
    "    \n",
    "    precision[0] += res['0']['precision']\n",
    "    precision[1] += res['1']['precision']\n",
    "    precision[2] += res['2']['precision']\n",
    "    precision[3] += res['3']['precision']\n",
    "    \n",
    "    recall[0] += res['0']['recall']\n",
    "    recall[1] += res['1']['recall']\n",
    "    recall[2] += res['2']['recall']\n",
    "    recall[3] += res['3']['recall']\n",
    "    \n",
    "    f_score[0] += res['0']['f1-score']\n",
    "    f_score[1] += res['1']['f1-score']\n",
    "    f_score[2] += res['2']['f1-score']\n",
    "    f_score[3] += res['3']['f1-score']\n",
    "    \n",
    "    accuracy += res['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.6666666666666665, 5.994795674379037, 7.21297815585316, 5.4290836979287365]\n",
      "[1.566305753070459, 6.606567844638649, 7.426028947211191, 5.638014782375247]\n",
      "[2.131085603049528, 6.252236032006898, 7.236260503584873, 5.437528735682273]\n",
      "6.290938569627094\n"
     ]
    }
   ],
   "source": [
    "print(precision)\n",
    "print(recall)\n",
    "print(f_score)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36666666666666664, 0.5994795674379036, 0.721297815585316, 0.5429083697928736]\n",
      "[0.15663057530704588, 0.6606567844638649, 0.7426028947211191, 0.5638014782375247]\n",
      "[0.21310856030495282, 0.6252236032006898, 0.7236260503584873, 0.5437528735682273]\n",
      "0.6290938569627094\n",
      "[[2.3, 4.1, 4.6, 4.8], [0.7, 29.5, 8.3, 6.0], [0.4, 9.3, 58.3, 10.4], [1.1, 6.4, 11.7, 24.9]]\n"
     ]
    }
   ],
   "source": [
    "# final performance values\n",
    "print([x/10 for x in precision])\n",
    "print([x/10 for x in recall])\n",
    "print([x/10 for x in f_score])\n",
    "print(accuracy/10)\n",
    "print([[item / 10 for item in subl] for subl in confusion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
